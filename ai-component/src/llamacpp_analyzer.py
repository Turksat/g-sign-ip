import time
from typing import List, Dict, Any
from llama_cpp import Llama


class LlamaCppAnalyzer:
    def __init__(self, model_config: Dict[str, Any]):
        self.model_config = model_config
        self.llm = None
        self._load_model()

    def _load_model(self):
        active_model = self.model_config.get('active_model', 'qwen3_30b')
        models = self.model_config.get('models', {})
        
        if active_model not in models:
            raise ValueError(f"Active model '{active_model}' not found in models configuration")
        
        model_config = models[active_model]
        print(f"Loading LlamaCpp model: {active_model} - {model_config.get('repo_id', 'Unknown')}")
        
        try:
            repo_id = model_config.get('repo_id')
            filename = model_config.get('filename')
            n_ctx = model_config.get('n_ctx', 4096)
            n_gpu_layers = model_config.get('n_gpu_layers', -1)
            n_threads = model_config.get('n_threads', None)
            verbose = model_config.get('verbose', False)
            
            if not repo_id or not filename:
                raise ValueError("Both 'repo_id' and 'filename' must be specified in model config")
            
            self.llm = Llama.from_pretrained(
                repo_id=repo_id,
                filename=filename,
                n_ctx=n_ctx,
                n_gpu_layers=n_gpu_layers,
                n_threads=n_threads,
                verbose=verbose
            )
            
            self.generation_config = model_config.get('generation', {})
            
            print(f"LlamaCpp model '{active_model}' loaded successfully.")
            
        except Exception as e:
            print(f"Error loading LlamaCpp model: {e}")
            raise RuntimeError(f"Could not load LlamaCpp model: {e}")

    def generate_analysis(self, prompt: str) -> str:
        if not self.llm:
            return "LlamaCpp analysis is unavailable - model not loaded."
        
        messages = [
            {
                "role": "user", 
                "content": prompt
            }
        ]

        try:
            print("Generating LlamaCpp analysis...")
            start_time = time.time()
            
            response = self.llm.create_chat_completion(
                messages=messages,
                max_tokens=self.generation_config.get('max_tokens', 1000),
                temperature=self.generation_config.get('temperature', 0.7),
                min_p=self.generation_config.get('min_p', 0.0),
                top_p=self.generation_config.get('top_p', 0.95),
                top_k=self.generation_config.get('top_k', 40),
                repeat_penalty=self.generation_config.get('repeat_penalty', 1.1),
                stop=self.generation_config.get('stop_sequences', [])
            )
            
            end_time = time.time()
            response_time = end_time - start_time
            
            print(f"LlamaCpp analysis completed in {response_time:.2f} seconds.")
            
            if response and 'choices' in response and len(response['choices']) > 0:
                analysis_text = response['choices'][0]['message']['content']
                return analysis_text.strip()
            else:
                return "No valid analysis generated by LlamaCpp model."
                
        except Exception as e:
            print(f"LlamaCpp analysis generation failed: {e}")
            return f"LlamaCpp analysis failed due to an error: {e}"

    def __del__(self):
        """Clean up resources when the analyzer is destroyed."""
        if hasattr(self, 'llm') and self.llm:
            del self.llm
